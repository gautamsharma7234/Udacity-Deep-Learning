{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - CNN\n",
    "\n",
    "Previously in 2_fullyconnected.ipynb and 3_regularization.ipynb (also 2 more assignments part of 3_regularization.ipynb), we trained fully connected networks to classify notMNIST characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "\n",
    "* convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "* labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(    \n",
    "            [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "        [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "        [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "        [num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "    \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.178333\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 11.9%\n",
      "Minibatch loss at step 50: 1.413656\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 51.8%\n",
      "Minibatch loss at step 100: 0.852075\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 150: 0.518346\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 200: 1.008632\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 250: 1.155398\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 300: 0.364634\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 350: 0.684691\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 400: 0.225201\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 450: 0.837756\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 500: 0.859082\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 550: 1.039811\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 600: 0.364684\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 650: 0.847205\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 700: 1.061313\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 750: 0.050264\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 800: 0.677360\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 850: 0.900306\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 900: 0.691850\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 950: 0.647620\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 1000: 0.453446\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 89.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(    \n",
    "            [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "        [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "        [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "        [num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "    \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        hidden = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        # data ---> conv2d | layer1 ---> relu --> max_pool\n",
    "        #\n",
    "        # data input tensor:      [16, 28, 28, 1]     4D tensor: (batch, h, w, ch)\n",
    "        # layer1 filter tensor:   [5, 5, 1, 16]\n",
    "        # conv2d output tensor:   [16, 28, 28, 16]\n",
    "        # relu output tensor:     [16, 28, 28, 16]\n",
    "        # max_pool output tensor: [16, 14, 14 16]\n",
    "        \n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        hidden = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2 ,2, 1], padding='SAME')\n",
    "        # hidden ---> conv2d | layer2 ---> relu --> max_pool\n",
    "        #\n",
    "        # hidden input tensor:    [16, 14, 14, 16]\n",
    "        # layer2 filter tensor:   [5, 5, 16, 16]\n",
    "        # conv2d output tensor:   [16, 14, 14, 16]\n",
    "        # relu output tensor:     [16, 14, 14, 16]\n",
    "        # max_pool output tensor: [16, 7, 7, 16]\n",
    "        \n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.108681\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.0%\n",
      "\n",
      "Minibatch loss at step 50: 1.858853\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 40.4%\n",
      "\n",
      "Minibatch loss at step 100: 1.461992\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 64.5%\n",
      "\n",
      "Minibatch loss at step 150: 0.496684\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 72.8%\n",
      "\n",
      "Minibatch loss at step 200: 1.049004\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 76.3%\n",
      "\n",
      "Minibatch loss at step 250: 1.479828\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 76.3%\n",
      "\n",
      "Minibatch loss at step 300: 0.396371\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.7%\n",
      "\n",
      "Minibatch loss at step 350: 0.520083\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.8%\n",
      "\n",
      "Minibatch loss at step 400: 0.209186\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.0%\n",
      "\n",
      "Minibatch loss at step 450: 0.768083\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.8%\n",
      "\n",
      "Minibatch loss at step 500: 0.684450\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.7%\n",
      "\n",
      "Minibatch loss at step 550: 0.689444\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.0%\n",
      "\n",
      "Minibatch loss at step 600: 0.311979\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.0%\n",
      "\n",
      "Minibatch loss at step 650: 0.980849\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "\n",
      "Minibatch loss at step 700: 0.846904\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.9%\n",
      "\n",
      "Minibatch loss at step 750: 0.094664\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.4%\n",
      "\n",
      "Minibatch loss at step 800: 0.799030\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.2%\n",
      "\n",
      "Minibatch loss at step 850: 0.848168\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.3%\n",
      "\n",
      "Minibatch loss at step 900: 0.685612\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.5%\n",
      "\n",
      "Minibatch loss at step 950: 0.541610\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.5%\n",
      "\n",
      "Minibatch loss at step 1000: 0.295122\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.1%\n",
      "\n",
      "Test accuracy: 90.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%\\n' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The accuracy of above classifier can be increased with more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic LeNet5 architecture, adding Dropout, and/or adding learning rate decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "Step 0 (epoch 0.00), 6.6 ms\n",
      "Minibatch loss: 9.657, learning rate: 0.010000\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 18.9%\n",
      "Step 100 (epoch 0.03), 348.3 ms\n",
      "Minibatch loss: 3.597, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 80.9%\n",
      "Step 200 (epoch 0.06), 349.7 ms\n",
      "Minibatch loss: 3.588, learning rate: 0.010000\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.1%\n",
      "Step 300 (epoch 0.10), 350.8 ms\n",
      "Minibatch loss: 3.895, learning rate: 0.010000\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 83.8%\n",
      "Step 400 (epoch 0.13), 351.7 ms\n",
      "Minibatch loss: 3.810, learning rate: 0.010000\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.1%\n",
      "Step 500 (epoch 0.16), 362.1 ms\n",
      "Minibatch loss: 3.351, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.5%\n",
      "Step 600 (epoch 0.19), 362.8 ms\n",
      "Minibatch loss: 3.475, learning rate: 0.010000\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.1%\n",
      "Step 700 (epoch 0.22), 350.8 ms\n",
      "Minibatch loss: 3.278, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.3%\n",
      "Step 800 (epoch 0.26), 349.3 ms\n",
      "Minibatch loss: 3.387, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.3%\n",
      "Step 900 (epoch 0.29), 350.2 ms\n",
      "Minibatch loss: 3.332, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.7%\n",
      "Step 1000 (epoch 0.32), 351.3 ms\n",
      "Minibatch loss: 3.237, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.1%\n",
      "Step 1100 (epoch 0.35), 349.8 ms\n",
      "Minibatch loss: 3.622, learning rate: 0.010000\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 87.5%\n",
      "Step 1200 (epoch 0.38), 349.7 ms\n",
      "Minibatch loss: 3.105, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.2%\n",
      "Step 1300 (epoch 0.42), 350.0 ms\n",
      "Minibatch loss: 3.023, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.5%\n",
      "Step 1400 (epoch 0.45), 350.6 ms\n",
      "Minibatch loss: 3.345, learning rate: 0.010000\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 87.8%\n",
      "Step 1500 (epoch 0.48), 348.9 ms\n",
      "Minibatch loss: 3.129, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.8%\n",
      "Step 1600 (epoch 0.51), 353.4 ms\n",
      "Minibatch loss: 2.886, learning rate: 0.010000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.0%\n",
      "Step 1700 (epoch 0.54), 354.0 ms\n",
      "Minibatch loss: 3.086, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.1%\n",
      "Step 1800 (epoch 0.58), 350.7 ms\n",
      "Minibatch loss: 3.017, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.0%\n",
      "Step 1900 (epoch 0.61), 349.6 ms\n",
      "Minibatch loss: 2.995, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.5%\n",
      "Step 2000 (epoch 0.64), 350.6 ms\n",
      "Minibatch loss: 2.961, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Step 2100 (epoch 0.67), 350.6 ms\n",
      "Minibatch loss: 2.728, learning rate: 0.010000\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 88.5%\n",
      "Step 2200 (epoch 0.70), 349.6 ms\n",
      "Minibatch loss: 2.915, learning rate: 0.010000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.9%\n",
      "Step 2300 (epoch 0.74), 349.5 ms\n",
      "Minibatch loss: 3.076, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.0%\n",
      "Step 2400 (epoch 0.77), 350.2 ms\n",
      "Minibatch loss: 2.833, learning rate: 0.010000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.9%\n",
      "Step 2500 (epoch 0.80), 351.2 ms\n",
      "Minibatch loss: 2.775, learning rate: 0.010000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.9%\n",
      "Step 2600 (epoch 0.83), 350.5 ms\n",
      "Minibatch loss: 2.698, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.0%\n",
      "Step 2700 (epoch 0.86), 350.9 ms\n",
      "Minibatch loss: 2.919, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Step 2800 (epoch 0.90), 350.3 ms\n",
      "Minibatch loss: 2.756, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.8%\n",
      "Step 2900 (epoch 0.93), 354.4 ms\n",
      "Minibatch loss: 2.822, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Step 3000 (epoch 0.96), 351.4 ms\n",
      "Minibatch loss: 2.680, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.1%\n",
      "Step 3100 (epoch 0.99), 352.9 ms\n",
      "Minibatch loss: 2.808, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.0%\n",
      "Step 3200 (epoch 1.02), 351.7 ms\n",
      "Minibatch loss: 2.580, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.2%\n",
      "Step 3300 (epoch 1.06), 350.8 ms\n",
      "Minibatch loss: 2.613, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Step 3400 (epoch 1.09), 350.7 ms\n",
      "Minibatch loss: 2.411, learning rate: 0.009500\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.3%\n",
      "Step 3500 (epoch 1.12), 350.7 ms\n",
      "Minibatch loss: 2.503, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "Step 3600 (epoch 1.15), 352.2 ms\n",
      "Minibatch loss: 2.683, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.2%\n",
      "Step 3700 (epoch 1.18), 350.2 ms\n",
      "Minibatch loss: 2.407, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.3%\n",
      "Step 3800 (epoch 1.22), 351.5 ms\n",
      "Minibatch loss: 2.650, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.4%\n",
      "Step 3900 (epoch 1.25), 349.9 ms\n",
      "Minibatch loss: 2.348, learning rate: 0.009500\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Step 4000 (epoch 1.28), 353.2 ms\n",
      "Minibatch loss: 2.486, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4100 (epoch 1.31), 351.8 ms\n",
      "Minibatch loss: 2.554, learning rate: 0.009500\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4200 (epoch 1.34), 353.8 ms\n",
      "Minibatch loss: 2.617, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.8%\n",
      "Step 4300 (epoch 1.38), 349.9 ms\n",
      "Minibatch loss: 2.368, learning rate: 0.009500\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4400 (epoch 1.41), 349.7 ms\n",
      "Minibatch loss: 2.322, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4500 (epoch 1.44), 349.6 ms\n",
      "Minibatch loss: 2.191, learning rate: 0.009500\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.6%\n",
      "Step 4600 (epoch 1.47), 349.9 ms\n",
      "Minibatch loss: 2.521, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4700 (epoch 1.50), 349.2 ms\n",
      "Minibatch loss: 2.422, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.6%\n",
      "Step 4800 (epoch 1.54), 349.8 ms\n",
      "Minibatch loss: 2.434, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4900 (epoch 1.57), 350.5 ms\n",
      "Minibatch loss: 2.143, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5000 (epoch 1.60), 352.6 ms\n",
      "Minibatch loss: 2.295, learning rate: 0.009500\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5100 (epoch 1.63), 352.9 ms\n",
      "Minibatch loss: 2.468, learning rate: 0.009500\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.8%\n",
      "Step 5200 (epoch 1.66), 352.7 ms\n",
      "Minibatch loss: 2.412, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5300 (epoch 1.70), 350.1 ms\n",
      "Minibatch loss: 2.347, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.2%\n",
      "Step 5400 (epoch 1.73), 348.3 ms\n",
      "Minibatch loss: 2.261, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5500 (epoch 1.76), 350.5 ms\n",
      "Minibatch loss: 2.147, learning rate: 0.009500\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5600 (epoch 1.79), 350.1 ms\n",
      "Minibatch loss: 2.099, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.1%\n",
      "Step 5700 (epoch 1.82), 349.7 ms\n",
      "Minibatch loss: 2.169, learning rate: 0.009500\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5800 (epoch 1.86), 349.6 ms\n",
      "Minibatch loss: 2.217, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5900 (epoch 1.89), 350.4 ms\n",
      "Minibatch loss: 2.049, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.9%\n",
      "Step 6000 (epoch 1.92), 350.5 ms\n",
      "Minibatch loss: 1.867, learning rate: 0.009500\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.0%\n",
      "Step 6100 (epoch 1.95), 349.7 ms\n",
      "Minibatch loss: 2.008, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.3%\n",
      "Step 6200 (epoch 1.98), 350.3 ms\n",
      "Minibatch loss: 2.086, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.3%\n",
      "Step 6300 (epoch 2.02), 349.2 ms\n",
      "Minibatch loss: 2.119, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.2%\n",
      "Step 6400 (epoch 2.05), 349.6 ms\n",
      "Minibatch loss: 2.040, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.3%\n",
      "Step 6500 (epoch 2.08), 350.3 ms\n",
      "Minibatch loss: 1.847, learning rate: 0.009025\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.1%\n",
      "Step 6600 (epoch 2.11), 349.3 ms\n",
      "Minibatch loss: 1.869, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Step 6700 (epoch 2.14), 349.1 ms\n",
      "Minibatch loss: 2.043, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.3%\n",
      "Step 6800 (epoch 2.18), 351.7 ms\n",
      "Minibatch loss: 1.769, learning rate: 0.009025\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Step 6900 (epoch 2.21), 350.2 ms\n",
      "Minibatch loss: 1.878, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.4%\n",
      "Step 7000 (epoch 2.24), 349.3 ms\n",
      "Minibatch loss: 1.916, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.4%\n",
      "Step 7100 (epoch 2.27), 350.5 ms\n",
      "Minibatch loss: 1.873, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.3%\n",
      "Step 7200 (epoch 2.30), 348.2 ms\n",
      "Minibatch loss: 1.976, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.3%\n",
      "Step 7300 (epoch 2.34), 349.1 ms\n",
      "Minibatch loss: 1.913, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.5%\n",
      "Step 7400 (epoch 2.37), 349.7 ms\n",
      "Minibatch loss: 2.119, learning rate: 0.009025\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 90.3%\n",
      "Step 7500 (epoch 2.40), 348.9 ms\n",
      "Minibatch loss: 1.914, learning rate: 0.009025\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.8%\n",
      "Step 7600 (epoch 2.43), 350.6 ms\n",
      "Minibatch loss: 1.893, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.6%\n",
      "Step 7700 (epoch 2.46), 349.0 ms\n",
      "Minibatch loss: 1.796, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.6%\n",
      "Step 7800 (epoch 2.50), 349.1 ms\n",
      "Minibatch loss: 1.746, learning rate: 0.009025\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.4%\n",
      "Step 7900 (epoch 2.53), 349.6 ms\n",
      "Minibatch loss: 1.791, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.6%\n",
      "Step 8000 (epoch 2.56), 350.0 ms\n",
      "Minibatch loss: 1.792, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Step 8100 (epoch 2.59), 349.2 ms\n",
      "Minibatch loss: 1.790, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.7%\n",
      "Step 8200 (epoch 2.62), 349.7 ms\n",
      "Minibatch loss: 1.657, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Step 8300 (epoch 2.66), 352.0 ms\n",
      "Minibatch loss: 1.676, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Step 8400 (epoch 2.69), 351.6 ms\n",
      "Minibatch loss: 1.897, learning rate: 0.009025\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 91.0%\n",
      "Step 8500 (epoch 2.72), 349.7 ms\n",
      "Minibatch loss: 1.910, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.8%\n",
      "Step 8600 (epoch 2.75), 350.6 ms\n",
      "Minibatch loss: 1.678, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Step 8700 (epoch 2.78), 349.2 ms\n",
      "Minibatch loss: 1.836, learning rate: 0.009025\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 90.8%\n",
      "Step 8800 (epoch 2.82), 350.6 ms\n",
      "Minibatch loss: 1.745, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.6%\n",
      "Step 8900 (epoch 2.85), 350.1 ms\n",
      "Minibatch loss: 1.637, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.6%\n",
      "Step 9000 (epoch 2.88), 348.6 ms\n",
      "Minibatch loss: 1.531, learning rate: 0.009025\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.0%\n",
      "Step 9100 (epoch 2.91), 348.6 ms\n",
      "Minibatch loss: 1.702, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.8%\n",
      "Step 9200 (epoch 2.94), 348.9 ms\n",
      "Minibatch loss: 1.647, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.9%\n",
      "Step 9300 (epoch 2.98), 350.3 ms\n",
      "Minibatch loss: 1.657, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.0%\n",
      "Step 9400 (epoch 3.01), 349.4 ms\n",
      "Minibatch loss: 1.592, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Step 9500 (epoch 3.04), 350.0 ms\n",
      "Minibatch loss: 1.633, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Step 9600 (epoch 3.07), 350.3 ms\n",
      "Minibatch loss: 1.509, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.1%\n",
      "Step 9700 (epoch 3.10), 348.6 ms\n",
      "Minibatch loss: 1.420, learning rate: 0.008574\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.1%\n",
      "Step 9800 (epoch 3.14), 351.4 ms\n",
      "Minibatch loss: 1.559, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.2%\n",
      "Step 9900 (epoch 3.17), 350.0 ms\n",
      "Minibatch loss: 1.616, learning rate: 0.008574\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.9%\n",
      "Step 10000 (epoch 3.20), 349.7 ms\n",
      "Minibatch loss: 1.403, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.7%\n",
      "Step 10100 (epoch 3.23), 351.8 ms\n",
      "Minibatch loss: 1.537, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Step 10200 (epoch 3.26), 356.4 ms\n",
      "Minibatch loss: 1.379, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.1%\n",
      "Step 10300 (epoch 3.30), 351.8 ms\n",
      "Minibatch loss: 1.501, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.9%\n",
      "Step 10400 (epoch 3.33), 351.6 ms\n",
      "Minibatch loss: 1.476, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.1%\n",
      "Step 10500 (epoch 3.36), 352.8 ms\n",
      "Minibatch loss: 1.563, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.1%\n",
      "Step 10600 (epoch 3.39), 351.4 ms\n",
      "Minibatch loss: 1.323, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.4%\n",
      "Step 10700 (epoch 3.42), 351.0 ms\n",
      "Minibatch loss: 1.448, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.1%\n",
      "Step 10800 (epoch 3.46), 348.9 ms\n",
      "Minibatch loss: 1.695, learning rate: 0.008574\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 91.0%\n",
      "Step 10900 (epoch 3.49), 349.3 ms\n",
      "Minibatch loss: 1.440, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.0%\n",
      "Step 11000 (epoch 3.52), 347.5 ms\n",
      "Minibatch loss: 1.389, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.1%\n",
      "Step 11100 (epoch 3.55), 15823.4 ms\n",
      "Minibatch loss: 1.345, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.2%\n",
      "Step 11200 (epoch 3.58), 390.8 ms\n",
      "Minibatch loss: 1.373, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.3%\n",
      "Step 11300 (epoch 3.62), 354.7 ms\n",
      "Minibatch loss: 1.404, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.3%\n",
      "Step 11400 (epoch 3.65), 351.0 ms\n",
      "Minibatch loss: 1.291, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.2%\n",
      "Step 11500 (epoch 3.68), 352.1 ms\n",
      "Minibatch loss: 1.419, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.3%\n",
      "Step 11600 (epoch 3.71), 350.3 ms\n",
      "Minibatch loss: 1.585, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.5%\n",
      "Step 11700 (epoch 3.74), 349.6 ms\n",
      "Minibatch loss: 1.362, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.3%\n",
      "Step 11800 (epoch 3.78), 352.0 ms\n",
      "Minibatch loss: 1.304, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.4%\n",
      "Step 11900 (epoch 3.81), 351.5 ms\n",
      "Minibatch loss: 1.347, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.4%\n",
      "Step 12000 (epoch 3.84), 350.1 ms\n",
      "Minibatch loss: 1.357, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Step 12100 (epoch 3.87), 350.2 ms\n",
      "Minibatch loss: 1.188, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.6%\n",
      "Step 12200 (epoch 3.90), 350.3 ms\n",
      "Minibatch loss: 1.281, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.4%\n",
      "Step 12300 (epoch 3.94), 351.7 ms\n",
      "Minibatch loss: 1.195, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.5%\n",
      "Step 12400 (epoch 3.97), 389.5 ms\n",
      "Minibatch loss: 1.327, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.1%\n",
      "Step 12500 (epoch 4.00), 402.9 ms\n",
      "Minibatch loss: 1.146, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.2%\n",
      "Step 12600 (epoch 4.03), 370.2 ms\n",
      "Minibatch loss: 1.368, learning rate: 0.008145\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.4%\n",
      "Step 12700 (epoch 4.06), 373.0 ms\n",
      "Minibatch loss: 1.216, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.3%\n",
      "Step 12800 (epoch 4.10), 367.2 ms\n",
      "Minibatch loss: 1.289, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Step 12900 (epoch 4.13), 413.4 ms\n",
      "Minibatch loss: 1.083, learning rate: 0.008145\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.3%\n",
      "Step 13000 (epoch 4.16), 463.4 ms\n",
      "Minibatch loss: 1.406, learning rate: 0.008145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.2%\n",
      "Step 13100 (epoch 4.19), 433.9 ms\n",
      "Minibatch loss: 1.265, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13200 (epoch 4.22), 405.3 ms\n",
      "Minibatch loss: 1.309, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13300 (epoch 4.26), 378.3 ms\n",
      "Minibatch loss: 1.069, learning rate: 0.008145\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.3%\n",
      "Step 13400 (epoch 4.29), 443.9 ms\n",
      "Minibatch loss: 1.054, learning rate: 0.008145\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13500 (epoch 4.32), 399.4 ms\n",
      "Minibatch loss: 1.066, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13600 (epoch 4.35), 351.4 ms\n",
      "Minibatch loss: 1.272, learning rate: 0.008145\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13700 (epoch 4.38), 351.5 ms\n",
      "Minibatch loss: 1.230, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13800 (epoch 4.42), 351.0 ms\n",
      "Minibatch loss: 1.132, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Step 13900 (epoch 4.45), 351.5 ms\n",
      "Minibatch loss: 1.171, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Step 14000 (epoch 4.48), 351.1 ms\n",
      "Minibatch loss: 1.032, learning rate: 0.008145\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.5%\n",
      "Step 14100 (epoch 4.51), 350.8 ms\n",
      "Minibatch loss: 1.121, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.5%\n",
      "Step 14200 (epoch 4.54), 351.5 ms\n",
      "Minibatch loss: 1.177, learning rate: 0.008145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.4%\n",
      "Step 14300 (epoch 4.58), 431.8 ms\n",
      "Minibatch loss: 1.136, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.4%\n",
      "Step 14400 (epoch 4.61), 403.6 ms\n",
      "Minibatch loss: 1.021, learning rate: 0.008145\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.5%\n",
      "Step 14500 (epoch 4.64), 400.4 ms\n",
      "Minibatch loss: 1.102, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Step 14600 (epoch 4.67), 426.5 ms\n",
      "Minibatch loss: 1.228, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Step 14700 (epoch 4.70), 385.0 ms\n",
      "Minibatch loss: 0.938, learning rate: 0.008145\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.6%\n",
      "Step 14800 (epoch 4.74), 389.7 ms\n",
      "Minibatch loss: 1.274, learning rate: 0.008145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.8%\n",
      "Step 14900 (epoch 4.77), 393.3 ms\n",
      "Minibatch loss: 1.055, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15000 (epoch 4.80), 393.6 ms\n",
      "Minibatch loss: 1.039, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Step 15100 (epoch 4.83), 462.5 ms\n",
      "Minibatch loss: 1.044, learning rate: 0.008145\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15200 (epoch 4.86), 365.2 ms\n",
      "Minibatch loss: 1.085, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15300 (epoch 4.90), 410.1 ms\n",
      "Minibatch loss: 1.178, learning rate: 0.008145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15400 (epoch 4.93), 472.0 ms\n",
      "Minibatch loss: 1.040, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Step 15500 (epoch 4.96), 442.5 ms\n",
      "Minibatch loss: 1.044, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Step 15600 (epoch 4.99), 416.3 ms\n",
      "Minibatch loss: 1.044, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Step 15700 (epoch 5.02), 405.7 ms\n",
      "Minibatch loss: 1.144, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15800 (epoch 5.06), 389.2 ms\n",
      "Minibatch loss: 1.062, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15900 (epoch 5.09), 387.5 ms\n",
      "Minibatch loss: 0.887, learning rate: 0.007738\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.8%\n",
      "Step 16000 (epoch 5.12), 386.4 ms\n",
      "Minibatch loss: 1.105, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Step 16100 (epoch 5.15), 394.7 ms\n",
      "Minibatch loss: 1.040, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Step 16200 (epoch 5.18), 395.5 ms\n",
      "Minibatch loss: 0.931, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Step 16300 (epoch 5.22), 386.8 ms\n",
      "Minibatch loss: 1.002, learning rate: 0.007738\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.8%\n",
      "Step 16400 (epoch 5.25), 393.6 ms\n",
      "Minibatch loss: 1.018, learning rate: 0.007738\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Step 16500 (epoch 5.28), 413.3 ms\n",
      "Minibatch loss: 0.970, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Step 16600 (epoch 5.31), 409.9 ms\n",
      "Minibatch loss: 1.056, learning rate: 0.007738\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.7%\n",
      "Step 16700 (epoch 5.34), 389.4 ms\n",
      "Minibatch loss: 1.120, learning rate: 0.007738\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.8%\n",
      "Step 16800 (epoch 5.38), 448.8 ms\n",
      "Minibatch loss: 1.022, learning rate: 0.007738\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.8%\n",
      "Step 16900 (epoch 5.41), 460.1 ms\n",
      "Minibatch loss: 1.076, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17000 (epoch 5.44), 427.5 ms\n",
      "Minibatch loss: 0.997, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17100 (epoch 5.47), 466.8 ms\n",
      "Minibatch loss: 1.079, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Step 17200 (epoch 5.50), 471.6 ms\n",
      "Minibatch loss: 0.862, learning rate: 0.007738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.5%\n",
      "Step 17300 (epoch 5.54), 374.8 ms\n",
      "Minibatch loss: 0.964, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Step 17400 (epoch 5.57), 399.2 ms\n",
      "Minibatch loss: 0.929, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17500 (epoch 5.60), 406.0 ms\n",
      "Minibatch loss: 0.893, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17600 (epoch 5.63), 461.6 ms\n",
      "Minibatch loss: 0.961, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17700 (epoch 5.66), 381.4 ms\n",
      "Minibatch loss: 0.876, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Step 17800 (epoch 5.70), 413.8 ms\n",
      "Minibatch loss: 0.927, learning rate: 0.007738\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Step 17900 (epoch 5.73), 485.7 ms\n",
      "Minibatch loss: 0.924, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Step 18000 (epoch 5.76), 393.8 ms\n",
      "Minibatch loss: 0.974, learning rate: 0.007738\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18100 (epoch 5.79), 375.4 ms\n",
      "Minibatch loss: 0.813, learning rate: 0.007738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Step 18200 (epoch 5.82), 516.9 ms\n",
      "Minibatch loss: 0.873, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 18300 (epoch 5.86), 469.6 ms\n",
      "Minibatch loss: 0.880, learning rate: 0.007738\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18400 (epoch 5.89), 382.1 ms\n",
      "Minibatch loss: 0.898, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Step 18500 (epoch 5.92), 434.0 ms\n",
      "Minibatch loss: 0.853, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18600 (epoch 5.95), 424.2 ms\n",
      "Minibatch loss: 0.826, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Step 18700 (epoch 5.98), 419.4 ms\n",
      "Minibatch loss: 0.776, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Step 18800 (epoch 6.02), 415.8 ms\n",
      "Minibatch loss: 0.888, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 18900 (epoch 6.05), 472.1 ms\n",
      "Minibatch loss: 0.909, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Step 19000 (epoch 6.08), 385.9 ms\n",
      "Minibatch loss: 0.814, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Step 19100 (epoch 6.11), 391.4 ms\n",
      "Minibatch loss: 0.967, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Step 19200 (epoch 6.14), 431.1 ms\n",
      "Minibatch loss: 0.739, learning rate: 0.007351\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Step 19300 (epoch 6.18), 433.2 ms\n",
      "Minibatch loss: 0.853, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Step 19400 (epoch 6.21), 371.4 ms\n",
      "Minibatch loss: 0.748, learning rate: 0.007351\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.9%\n",
      "Step 19500 (epoch 6.24), 377.3 ms\n",
      "Minibatch loss: 0.821, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Step 19600 (epoch 6.27), 352.2 ms\n",
      "Minibatch loss: 0.744, learning rate: 0.007351\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Step 19700 (epoch 6.30), 353.9 ms\n",
      "Minibatch loss: 0.918, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Step 19800 (epoch 6.34), 351.4 ms\n",
      "Minibatch loss: 0.744, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Step 19900 (epoch 6.37), 360.6 ms\n",
      "Minibatch loss: 0.771, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Step 20000 (epoch 6.40), 356.7 ms\n",
      "Minibatch loss: 0.707, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20100 (epoch 6.43), 351.7 ms\n",
      "Minibatch loss: 1.147, learning rate: 0.007351\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20200 (epoch 6.46), 355.7 ms\n",
      "Minibatch loss: 0.890, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20300 (epoch 6.50), 352.2 ms\n",
      "Minibatch loss: 0.882, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20400 (epoch 6.53), 353.0 ms\n",
      "Minibatch loss: 1.009, learning rate: 0.007351\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20500 (epoch 6.56), 352.0 ms\n",
      "Minibatch loss: 0.846, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Step 20600 (epoch 6.59), 351.2 ms\n",
      "Minibatch loss: 0.702, learning rate: 0.007351\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20700 (epoch 6.62), 351.4 ms\n",
      "Minibatch loss: 0.623, learning rate: 0.007351\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20800 (epoch 6.66), 351.5 ms\n",
      "Minibatch loss: 0.839, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20900 (epoch 6.69), 353.4 ms\n",
      "Minibatch loss: 0.794, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 21000 (epoch 6.72), 351.6 ms\n",
      "Minibatch loss: 0.812, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21100 (epoch 6.75), 351.3 ms\n",
      "Minibatch loss: 0.847, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21200 (epoch 6.78), 351.7 ms\n",
      "Minibatch loss: 0.684, learning rate: 0.007351\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21300 (epoch 6.82), 351.7 ms\n",
      "Minibatch loss: 0.693, learning rate: 0.007351\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21400 (epoch 6.85), 351.7 ms\n",
      "Minibatch loss: 0.749, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 21500 (epoch 6.88), 352.2 ms\n",
      "Minibatch loss: 0.816, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21600 (epoch 6.91), 352.2 ms\n",
      "Minibatch loss: 0.697, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21700 (epoch 6.94), 352.0 ms\n",
      "Minibatch loss: 0.728, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21800 (epoch 6.98), 352.4 ms\n",
      "Minibatch loss: 0.796, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Step 21900 (epoch 7.01), 352.6 ms\n",
      "Minibatch loss: 0.768, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Step 22000 (epoch 7.04), 352.7 ms\n",
      "Minibatch loss: 0.609, learning rate: 0.006983\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.2%\n",
      "Step 22100 (epoch 7.07), 352.9 ms\n",
      "Minibatch loss: 0.786, learning rate: 0.006983\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.2%\n",
      "Step 22200 (epoch 7.10), 352.1 ms\n",
      "Minibatch loss: 0.802, learning rate: 0.006983\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.3%\n",
      "Step 22300 (epoch 7.14), 351.3 ms\n",
      "Minibatch loss: 0.619, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 22400 (epoch 7.17), 351.0 ms\n",
      "Minibatch loss: 0.910, learning rate: 0.006983\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 92.1%\n",
      "Step 22500 (epoch 7.20), 352.9 ms\n",
      "Minibatch loss: 0.718, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Step 22600 (epoch 7.23), 352.1 ms\n",
      "Minibatch loss: 0.807, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.1%\n",
      "Step 22700 (epoch 7.26), 352.6 ms\n",
      "Minibatch loss: 0.872, learning rate: 0.006983\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.0%\n",
      "Step 22800 (epoch 7.30), 352.7 ms\n",
      "Minibatch loss: 0.689, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 22900 (epoch 7.33), 352.2 ms\n",
      "Minibatch loss: 0.755, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Step 23000 (epoch 7.36), 352.5 ms\n",
      "Minibatch loss: 0.772, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 23100 (epoch 7.39), 354.7 ms\n",
      "Minibatch loss: 0.627, learning rate: 0.006983\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.2%\n",
      "Step 23200 (epoch 7.42), 352.6 ms\n",
      "Minibatch loss: 1.055, learning rate: 0.006983\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 91.9%\n",
      "Step 23300 (epoch 7.46), 350.9 ms\n",
      "Minibatch loss: 0.715, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 23400 (epoch 7.49), 351.6 ms\n",
      "Minibatch loss: 0.641, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Step 23500 (epoch 7.52), 351.5 ms\n",
      "Minibatch loss: 0.788, learning rate: 0.006983\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.2%\n",
      "Step 23600 (epoch 7.55), 352.9 ms\n",
      "Minibatch loss: 0.589, learning rate: 0.006983\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.1%\n",
      "Step 23700 (epoch 7.58), 381.8 ms\n",
      "Minibatch loss: 0.527, learning rate: 0.006983\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.2%\n",
      "Step 23800 (epoch 7.62), 355.3 ms\n",
      "Minibatch loss: 0.646, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Step 23900 (epoch 7.65), 372.3 ms\n",
      "Minibatch loss: 0.632, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24000 (epoch 7.68), 379.7 ms\n",
      "Minibatch loss: 0.834, learning rate: 0.006983\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.1%\n",
      "Step 24100 (epoch 7.71), 374.6 ms\n",
      "Minibatch loss: 0.568, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24200 (epoch 7.74), 414.8 ms\n",
      "Minibatch loss: 0.640, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 24300 (epoch 7.78), 416.7 ms\n",
      "Minibatch loss: 0.695, learning rate: 0.006983\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24400 (epoch 7.81), 409.7 ms\n",
      "Minibatch loss: 0.624, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Step 24500 (epoch 7.84), 412.5 ms\n",
      "Minibatch loss: 0.974, learning rate: 0.006983\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.4%\n",
      "Step 24600 (epoch 7.87), 440.9 ms\n",
      "Minibatch loss: 0.651, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 24700 (epoch 7.90), 370.5 ms\n",
      "Minibatch loss: 0.501, learning rate: 0.006983\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24800 (epoch 7.94), 396.1 ms\n",
      "Minibatch loss: 0.764, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Step 24900 (epoch 7.97), 435.8 ms\n",
      "Minibatch loss: 0.593, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 25000 (epoch 8.00), 362.0 ms\n",
      "Minibatch loss: 0.726, learning rate: 0.006634\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25100 (epoch 8.03), 416.7 ms\n",
      "Minibatch loss: 0.829, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.4%\n",
      "Step 25200 (epoch 8.06), 412.1 ms\n",
      "Minibatch loss: 0.592, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25300 (epoch 8.10), 406.9 ms\n",
      "Minibatch loss: 0.600, learning rate: 0.006634\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25400 (epoch 8.13), 353.7 ms\n",
      "Minibatch loss: 0.763, learning rate: 0.006634\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.4%\n",
      "Step 25500 (epoch 8.16), 405.0 ms\n",
      "Minibatch loss: 0.583, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25600 (epoch 8.19), 352.5 ms\n",
      "Minibatch loss: 0.646, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25700 (epoch 8.22), 352.3 ms\n",
      "Minibatch loss: 0.556, learning rate: 0.006634\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.3%\n",
      "Step 25800 (epoch 8.26), 351.8 ms\n",
      "Minibatch loss: 0.614, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 25900 (epoch 8.29), 351.9 ms\n",
      "Minibatch loss: 0.775, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26000 (epoch 8.32), 350.8 ms\n",
      "Minibatch loss: 0.567, learning rate: 0.006634\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26100 (epoch 8.35), 352.1 ms\n",
      "Minibatch loss: 0.577, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26200 (epoch 8.38), 365.9 ms\n",
      "Minibatch loss: 0.603, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 26300 (epoch 8.42), 390.5 ms\n",
      "Minibatch loss: 0.624, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26400 (epoch 8.45), 353.4 ms\n",
      "Minibatch loss: 0.639, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26500 (epoch 8.48), 356.2 ms\n",
      "Minibatch loss: 0.682, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.4%\n",
      "Step 26600 (epoch 8.51), 370.8 ms\n",
      "Minibatch loss: 0.598, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.1%\n",
      "Step 26700 (epoch 8.54), 354.7 ms\n",
      "Minibatch loss: 0.526, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26800 (epoch 8.58), 351.6 ms\n",
      "Minibatch loss: 0.750, learning rate: 0.006634\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.5%\n",
      "Step 26900 (epoch 8.61), 350.9 ms\n",
      "Minibatch loss: 0.664, learning rate: 0.006634\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.4%\n",
      "Step 27000 (epoch 8.64), 351.7 ms\n",
      "Minibatch loss: 0.753, learning rate: 0.006634\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 92.5%\n",
      "Step 27100 (epoch 8.67), 352.5 ms\n",
      "Minibatch loss: 0.540, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 27200 (epoch 8.70), 350.6 ms\n",
      "Minibatch loss: 0.663, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.6%\n",
      "Step 27300 (epoch 8.74), 351.3 ms\n",
      "Minibatch loss: 0.472, learning rate: 0.006634\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Step 27400 (epoch 8.77), 351.5 ms\n",
      "Minibatch loss: 0.583, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 27500 (epoch 8.80), 350.8 ms\n",
      "Minibatch loss: 0.658, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Step 27600 (epoch 8.83), 351.7 ms\n",
      "Minibatch loss: 0.487, learning rate: 0.006634\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.5%\n",
      "Step 27700 (epoch 8.86), 350.3 ms\n",
      "Minibatch loss: 0.636, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Step 27800 (epoch 8.90), 351.7 ms\n",
      "Minibatch loss: 0.497, learning rate: 0.006634\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Step 27900 (epoch 8.93), 353.0 ms\n",
      "Minibatch loss: 0.708, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.5%\n",
      "Step 28000 (epoch 8.96), 350.6 ms\n",
      "Minibatch loss: 0.597, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 28100 (epoch 8.99), 351.5 ms\n",
      "Minibatch loss: 0.645, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.5%\n",
      "Step 28200 (epoch 9.02), 351.8 ms\n",
      "Minibatch loss: 0.622, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.6%\n",
      "Step 28300 (epoch 9.06), 351.3 ms\n",
      "Minibatch loss: 0.527, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 28400 (epoch 9.09), 351.3 ms\n",
      "Minibatch loss: 0.489, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.3%\n",
      "Step 28500 (epoch 9.12), 352.4 ms\n",
      "Minibatch loss: 0.552, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28600 (epoch 9.15), 352.1 ms\n",
      "Minibatch loss: 0.541, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28700 (epoch 9.18), 350.8 ms\n",
      "Minibatch loss: 0.606, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28800 (epoch 9.22), 353.3 ms\n",
      "Minibatch loss: 0.481, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28900 (epoch 9.25), 394.1 ms\n",
      "Minibatch loss: 0.570, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 29000 (epoch 9.28), 396.1 ms\n",
      "Minibatch loss: 0.449, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.6%\n",
      "Step 29100 (epoch 9.31), 363.7 ms\n",
      "Minibatch loss: 0.498, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29200 (epoch 9.34), 374.0 ms\n",
      "Minibatch loss: 0.434, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29300 (epoch 9.38), 374.0 ms\n",
      "Minibatch loss: 0.414, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29400 (epoch 9.41), 361.6 ms\n",
      "Minibatch loss: 0.480, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Step 29500 (epoch 9.44), 364.0 ms\n",
      "Minibatch loss: 0.547, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 29600 (epoch 9.47), 356.5 ms\n",
      "Minibatch loss: 0.530, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29700 (epoch 9.50), 353.5 ms\n",
      "Minibatch loss: 0.656, learning rate: 0.006302\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29800 (epoch 9.54), 351.7 ms\n",
      "Minibatch loss: 0.547, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29900 (epoch 9.57), 357.5 ms\n",
      "Minibatch loss: 0.593, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 30000 (epoch 9.60), 351.6 ms\n",
      "Minibatch loss: 0.539, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 30100 (epoch 9.63), 354.6 ms\n",
      "Minibatch loss: 0.508, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 30200 (epoch 9.66), 353.7 ms\n",
      "Minibatch loss: 0.543, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Step 30300 (epoch 9.70), 351.5 ms\n",
      "Minibatch loss: 0.503, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Step 30400 (epoch 9.73), 351.7 ms\n",
      "Minibatch loss: 0.561, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 30500 (epoch 9.76), 368.3 ms\n",
      "Minibatch loss: 0.730, learning rate: 0.006302\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.3%\n",
      "Step 30600 (epoch 9.79), 385.5 ms\n",
      "Minibatch loss: 0.440, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.6%\n",
      "Step 30700 (epoch 9.82), 385.5 ms\n",
      "Minibatch loss: 0.656, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.4%\n",
      "Step 30800 (epoch 9.86), 366.9 ms\n",
      "Minibatch loss: 0.439, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Step 30900 (epoch 9.89), 377.2 ms\n",
      "Minibatch loss: 0.529, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.5%\n",
      "Step 31000 (epoch 9.92), 370.2 ms\n",
      "Minibatch loss: 0.601, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.5%\n",
      "Step 31100 (epoch 9.95), 351.4 ms\n",
      "Minibatch loss: 0.810, learning rate: 0.006302\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 92.5%\n",
      "Step 31200 (epoch 9.98), 358.5 ms\n",
      "Minibatch loss: 0.470, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Test accuracy: 97.2%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "batch_size=64\n",
    "PIXEL_DEPTH = 255\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_FREQUENCY = 100  # Number of steps between evaluations.\n",
    "\n",
    "def lenet(argv=None):  # pylint: disable=unused-argument\n",
    "    train_size = train_labels.shape[0]\n",
    "\n",
    "    # This is where training samples and labels are fed to the graph.\n",
    "    # These placeholder nodes will be fed a batch of training data at each\n",
    "    # training step using the {feed_dict} argument to the Run() call below.\n",
    "    train_data_node = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=(batch_size, image_size, image_size, num_channels))\n",
    "    train_labels_node = tf.placeholder(\n",
    "                                    tf.float32,\n",
    "                                    shape=(batch_size, num_labels))\n",
    "    eval_data = tf.placeholder(\n",
    "                            tf.float32,\n",
    "                            shape=(EVAL_BATCH_SIZE, image_size, image_size, num_channels))\n",
    "\n",
    "    # The variables below hold all the trainable weights. They are passed an\n",
    "    # initial value which will be assigned when when we call:\n",
    "    # {tf.initialize_all_variables().run()}\n",
    "    conv1_weights = tf.Variable(\n",
    "            tf.truncated_normal([5, 5, num_channels, 32],  # 5x5 filter, depth 32.\n",
    "                                                    stddev=0.1,\n",
    "                                                    seed=SEED))\n",
    "    conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "    conv2_weights = tf.Variable(\n",
    "            tf.truncated_normal([5, 5, 32, 64],\n",
    "                                                    stddev=0.1,\n",
    "                                                    seed=SEED))\n",
    "    conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "    fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "            tf.truncated_normal(\n",
    "                    [image_size // 4 * image_size // 4 * 64, 512],\n",
    "                    stddev=0.1,\n",
    "                    seed=SEED))\n",
    "    fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "    fc2_weights = tf.Variable(\n",
    "            tf.truncated_normal([512, num_labels],\n",
    "                                                    stddev=0.1,\n",
    "                                                    seed=SEED))\n",
    "    fc2_biases = tf.Variable(tf.constant(0.1, shape=[num_labels]))\n",
    "\n",
    "    # We will replicate the model structure for the training subgraph, as well\n",
    "    # as the evaluation subgraphs, while sharing the trainable parameters.\n",
    "    def model(data, train=False):\n",
    "        \"\"\"The Model definition.\"\"\"\n",
    "        # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "        # the same size as the input). Note that {strides} is a 4D array whose\n",
    "        # shape matches the data layout: [image index, y, x, depth].\n",
    "        conv = tf.nn.conv2d(data,\n",
    "                                                conv1_weights,\n",
    "                                                strides=[1, 1, 1, 1],\n",
    "                                                padding='SAME')\n",
    "        # Bias and rectified linear non-linearity.\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "        # Max pooling. The kernel size spec {ksize} also follows the layout of\n",
    "        # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "        pool = tf.nn.max_pool(relu,\n",
    "                                                    ksize=[1, 2, 2, 1],\n",
    "                                                    strides=[1, 2, 2, 1],\n",
    "                                                    padding='SAME')\n",
    "        conv = tf.nn.conv2d(pool,\n",
    "                                                conv2_weights,\n",
    "                                                strides=[1, 1, 1, 1],\n",
    "                                                padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "        pool = tf.nn.max_pool(relu,\n",
    "                                                    ksize=[1, 2, 2, 1],\n",
    "                                                    strides=[1, 2, 2, 1],\n",
    "                                                    padding='SAME')\n",
    "        # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "        # fully connected layers.\n",
    "        pool_shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(\n",
    "                pool,\n",
    "                [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "        # Fully connected layer. Note that the '+' operation automatically\n",
    "        # broadcasts the biases.\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "        # Add a 50% dropout during training only. Dropout also scales\n",
    "        # activations such that no rescaling is needed at evaluation time.\n",
    "        if train:\n",
    "            hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "        return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "    # Training computation: logits + cross-entropy loss.\n",
    "    logits = model(train_data_node, True)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, train_labels_node))\n",
    "\n",
    "    # L2 regularization for the fully connected parameters.\n",
    "    regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                                    tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "    # Add the regularization term to the loss.\n",
    "    loss += 5e-4 * regularizers\n",
    "\n",
    "    # Optimizer: set up a variable that's incremented once per batch and\n",
    "    # controls the learning rate decay.\n",
    "    batch = tf.Variable(0)\n",
    "    # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "            0.01,                # Base learning rate.\n",
    "            batch * batch_size,  # Current index into the dataset.\n",
    "            train_size,          # Decay step.\n",
    "            0.95,                # Decay rate.\n",
    "            staircase=True)\n",
    "    # Use simple momentum for the optimization.\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)\n",
    "\n",
    "    # Predictions for the current training minibatch.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Predictions for the test and validation, which we'll compute less often.\n",
    "    eval_prediction = tf.nn.softmax(model(eval_data))\n",
    "\n",
    "    # Small utility function to evaluate a dataset by feeding batches of data to\n",
    "    # {eval_data} and pulling the results from {eval_predictions}.\n",
    "    # Saves memory and enables this to run on smaller GPUs.\n",
    "    def eval_in_batches(data, sess):\n",
    "        \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n",
    "        size = data.shape[0]\n",
    "        if size < EVAL_BATCH_SIZE:\n",
    "            raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "        predictions = np.ndarray(shape=(size, num_labels), dtype=np.float32)\n",
    "        for begin in xrange(0, size, EVAL_BATCH_SIZE):\n",
    "            end = begin + EVAL_BATCH_SIZE\n",
    "            if end <= size:\n",
    "                predictions[begin:end, :] = sess.run(\n",
    "                        eval_prediction,\n",
    "                        feed_dict={eval_data: data[begin:end, ...]})\n",
    "            else:\n",
    "                batch_predictions = sess.run(\n",
    "                        eval_prediction,\n",
    "                        feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
    "                predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
    "        return predictions\n",
    "\n",
    "    # Create a local session to run the training.\n",
    "    start_time = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        # Run all the initializers to prepare the trainable parameters.\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Initialized!')\n",
    "        # Loop through training steps.\n",
    "        for step in xrange(int(NUM_EPOCHS * train_size) // batch_size):\n",
    "            # Compute the offset of the current minibatch in the data.\n",
    "            # Note that we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_size - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), ...]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "            # This dictionary maps the batch data (as a numpy array) to the\n",
    "            # node in the graph is should be fed to.\n",
    "            feed_dict = {train_data_node: batch_data,\n",
    "                                     train_labels_node: batch_labels}\n",
    "            # Run the graph and fetch some of the nodes.\n",
    "            _, l, lr, predictions = sess.run(\n",
    "                    [optimizer, loss, learning_rate, train_prediction],\n",
    "                    feed_dict=feed_dict)\n",
    "            if step % EVAL_FREQUENCY == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                start_time = time.time()\n",
    "                print('Step %d (epoch %.2f), %.1f ms' %\n",
    "                            (step, float(step) * batch_size / train_size,\n",
    "                             1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "                print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                        eval_in_batches(valid_dataset, sess), valid_labels))\n",
    "                sys.stdout.flush()\n",
    "        # Finally print the result!\n",
    "        test_error = accuracy(eval_in_batches(test_dataset, sess), test_labels)\n",
    "        print('Test accuracy: %.1f%%' % test_error)\n",
    "        \n",
    "lenet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy - 97.2%\n",
    "### This could be increased with more epochs. But due to time constraints, I did with only 10 epochs"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tf-env-test]",
   "language": "python",
   "name": "Python [tf-env-test]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
